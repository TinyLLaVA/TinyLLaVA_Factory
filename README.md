<h2 align="center"> <a href="https://arxiv.org/abs/2402.14289">TinyLLaVA: A Framework of Small-scale Large Multimodal Models</a>

<h5 align="center">

[![hf_space](https://img.shields.io/badge/ðŸ¤—-%20Open%20In%20HF-blue.svg)](https://huggingface.co/bczhou/TinyLLaVA-3.1B) [![arXiv](https://img.shields.io/badge/Arxiv-2402.14289-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2402.14289) [![License](https://img.shields.io/badge/License-Apache%202.0-yellow)](https://github.com/PKU-YuanGroup/MoE-LLaVA/blob/main/LICENSE) 


## &#x1F389; News

* **[2024.02.23]**  We have released the eval code and eval scripts.
* **[2024.02.21]**  Creating the [TinyLLavaBench](https://github.com/DLCV-BUAA/TinyLLavaBench) repository on GitHub and technique report availalbe [here](https://arxiv.org/abs/2402.14289).
* **[2024.01.11]**  Our fist model [TinyLLaVA-1.4B](https://huggingface.co/bczhou/tiny-llava-v1-hf) is out

## &#x231B; TODO
- [x] We will release the eval code and weights in the afternoon(2024.2.23).
### &#x1F525; High performance, but with fewer parameters

- Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL.

## &#x1F433; Model Zoo
### Legacy
> https://huggingface.co/bczhou/tiny-llava-v1-hf

### Pretrain Model
- [TinyLLaVA-3.1B](https://huggingface.co/bczhou/TinyLLaVA-3.1B)


## &#x1F527; Requirements and Installation

We recommend the requirements as follows.


## &#x270F; Citation

If you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil:.

```BibTeX
@misc{zhou2024tinyllava,
      title={TinyLLaVA: A Framework of Small-scale Large Multimodal Models}, 
      author={Baichuan Zhou and Ying Hu and Xi Weng and Junlong Jia and Jie Luo and Xien Liu and Ji Wu and Lei Huang},
      year={2024},
      eprint={2402.14289},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


## &#x270A; Community efforts!
We built our project on the [LLaVA](https://github.com/haotian-liu/LLaVA) project, and the [ShareGPT4V](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V) project. Great work!

